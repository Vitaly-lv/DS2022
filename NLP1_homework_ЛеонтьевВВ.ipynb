{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vitaly-lv/DS2022/blob/main/NLP1_homework_%D0%9B%D0%B5%D0%BE%D0%BD%D1%82%D1%8C%D0%B5%D0%B2%D0%92%D0%92.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Задание 5: Подсчитайте количество разных слов\n",
        "\n"
      ],
      "metadata": {
        "id": "XXA7Fe_izuqh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIvhwE4zxX0s",
        "outputId": "6640e41f-698b-4809-81d5-10182a0babd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.2.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: bokeh in /usr/local/lib/python3.8/dist-packages (2.3.3)\n",
            "Collecting bokeh\n",
            "  Downloading bokeh-3.0.2-py3-none-any.whl (16.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.4 MB 133 kB/s \n",
            "\u001b[?25hCollecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.21.6)\n",
            "Collecting xyzservices>=2021.09.1\n",
            "  Downloading xyzservices-2022.9.0-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.8/dist-packages (from bokeh) (21.3)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.8/dist-packages (from bokeh) (6.0.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.8/dist-packages (from bokeh) (1.3.5)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.8/dist-packages (from bokeh) (2.11.3)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.8/dist-packages (from bokeh) (6.0)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.8/dist-packages (from bokeh) (7.1.2)\n",
            "Collecting contourpy>=1\n",
            "  Downloading contourpy-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (295 kB)\n",
            "\u001b[K     |████████████████████████████████| 295 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2>=2.9->bokeh) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=16.8->bokeh) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2->bokeh) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2->bokeh) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2->bokeh) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.8/dist-packages (from umap-learn) (1.0.2)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.8/dist-packages (from umap-learn) (0.56.4)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.8.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 52.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.49->umap-learn) (4.13.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.49->umap-learn) (0.39.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.49->umap-learn) (3.10.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=dbf9bc2ac0662893022ba52aa0b4f74c93ec0994b925f0174839f53025b553a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/3a/67/06a8950e053725912e6a8c42c4a3a241410f6487b8402542ea\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.8-py3-none-any.whl size=55513 sha256=17738d30c074fc074676e33a1a7d8c6222745f0bf7a325cb3af1eec9c47400f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/63/3a/29954bca1a27ba100ed8c27973a78cb71b43dc67aed62e80c3\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: xyzservices, pynndescent, contourpy, umap-learn, gensim, bokeh\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "  Attempting uninstall: bokeh\n",
            "    Found existing installation: bokeh 2.3.3\n",
            "    Uninstalling bokeh-2.3.3:\n",
            "      Successfully uninstalled bokeh-2.3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.12.1 requires bokeh<2.4.0,>=2.3.0, but you have bokeh 3.0.2 which is incompatible.\u001b[0m\n",
            "Successfully installed bokeh-3.0.2 contourpy-1.0.6 gensim-4.2.0 pynndescent-0.5.8 umap-learn-0.5.3 xyzservices-2022.9.0\n"
          ]
        }
      ],
      "source": [
        "#Установка нужных пакетов\n",
        "!pip install --upgrade nltk gensim bokeh umap-learn\n",
        "\n",
        "import itertools\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import umap\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# выгружаем датасет:\n",
        "!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt -nc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF9WPCtfxZR9",
        "outputId": "751bdef8-1884-48dd-9f9f-9176ebd65736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-07 06:33:31--  https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.2.18, 2620:100:6017:18::a27d:212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.2.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/obaitrix9jyu84r/quora.txt [following]\n",
            "--2022-12-07 06:33:31--  https://www.dropbox.com/s/dl/obaitrix9jyu84r/quora.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc698577a49b143f0e9f23ace7ed.dl.dropboxusercontent.com/cd/0/get/ByIjIhW7F8r_FPykG_NlLwT9iEE6T4fA4Wx56ODzSwh8tzOpmumI3MkdE7VXss53apDaT1Ii1MNvGRgWEvTDFrcRULKy2gAhOpqiPeVwg4DlAfEEkUV1m80TE12-lY2xc1ZuB7CYbK5ki2UWhrz50mnvSN_wxkOg8y3mQ63V9wdODw/file?dl=1# [following]\n",
            "--2022-12-07 06:33:32--  https://uc698577a49b143f0e9f23ace7ed.dl.dropboxusercontent.com/cd/0/get/ByIjIhW7F8r_FPykG_NlLwT9iEE6T4fA4Wx56ODzSwh8tzOpmumI3MkdE7VXss53apDaT1Ii1MNvGRgWEvTDFrcRULKy2gAhOpqiPeVwg4DlAfEEkUV1m80TE12-lY2xc1ZuB7CYbK5ki2UWhrz50mnvSN_wxkOg8y3mQ63V9wdODw/file?dl=1\n",
            "Resolving uc698577a49b143f0e9f23ace7ed.dl.dropboxusercontent.com (uc698577a49b143f0e9f23ace7ed.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to uc698577a49b143f0e9f23ace7ed.dl.dropboxusercontent.com (uc698577a49b143f0e9f23ace7ed.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33813903 (32M) [application/binary]\n",
            "Saving to: ‘./quora.txt’\n",
            "\n",
            "./quora.txt         100%[===================>]  32.25M  98.5MB/s    in 0.3s    \n",
            "\n",
            "2022-12-07 06:33:32 (98.5 MB/s) - ‘./quora.txt’ saved [33813903/33813903]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = list(open(\"./quora.txt\", encoding=\"utf-8\"))\n",
        "data[50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MaFpN9pvxtNg",
        "outputId": "f147645b-fee2-442d-8b7c-1ba21c5b0246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"What TV shows or books help you read people's body language?\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Токенизация и приведение к нижнему регистру\n",
        "tokenizer = WordPunctTokenizer()\n",
        "data_tok = []\n",
        "for x in data:\n",
        "    data_tok.append(tokenizer.tokenize(x.lower()))"
      ],
      "metadata": {
        "id": "HvXRbOKGx0l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l36-lCKfryyS",
        "outputId": "3264d523-4d41-478e-c07a-e70c6cf48217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Подсчет количество разных слов сначала лемматизация потом стэмминг"
      ],
      "metadata": {
        "id": "ACG4eFaeXa2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Лемматизация\n",
        "all_words = []\n",
        "for i in data_tok:\n",
        "  for j in i:\n",
        "    all_words.append(j)\n",
        "\n",
        "unique_words = set(all_words)\n",
        "all_lem = [lemmatizer.lemmatize(i) for i in unique_words] #Лемматизация только уникальных слов\n",
        "# Проверим есть ли повторяющиеся лемматизированные слова\n",
        "print(len(all_lem),len(set(all_lem))) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq5Z9qqoUu_S",
        "outputId": "16b43554-199a-4f20-c560-f4d8adc101bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87819 80303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_all_lem = set(all_lem) #так как есть повторяющиеся лемматизированные слова для стэмминга возьмем только уникальные"
      ],
      "metadata": {
        "id": "TdjE7BUXVBGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Стэмминг\n",
        "all_st = [ps.stem(i) for i in unique_all_lem]\n",
        "\n",
        "print(len(all_st),len(set(all_st)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqBe7l7wV-Qb",
        "outputId": "f62f6511-3e48-4564-96c9-e9458b418f41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80303 66835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Подсчет количество разных слов сначала стэмминг потом лемматизация  "
      ],
      "metadata": {
        "id": "ROjHotkAXzm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Стэмминг\n",
        "all_words = []\n",
        "for i in data_tok:\n",
        "  for j in i:\n",
        "    all_words.append(j)\n",
        "\n",
        "unique_words = set(all_words)\n",
        "# аналогично берем только уникальные слова\n",
        "all_st = [ps.stem(i) for i in unique_words]\n",
        "\n",
        "print(len(all_st),len(set(all_st)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dxr6WT6VYDeJ",
        "outputId": "b0772880-b48d-44a9-f812-42d6f698680f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87819 67026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_all_st = set(all_st) #для лемматизации возьмем только уникальные слова после стэмминга"
      ],
      "metadata": {
        "id": "M36q2JuqZQId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_lem = [lemmatizer.lemmatize(i) for i in unique_all_st]\n",
        "\n",
        "print(len(all_lem),len(set(all_lem)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3FeUYiGZhDz",
        "outputId": "7e1a27dd-3008-4384-cc1e-d1aada691414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "67026 66818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В первом случае (сначала лемматизация потом стемминг) получилось 66835 разных слов, во втором случае (сначала стэмминг потом лемматизация) получилось 66818 разных слов."
      ],
      "metadata": {
        "id": "2qIAS8yWX8kl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH7qx_irU4Y8"
      },
      "source": [
        "###ML1_1: \n",
        "https://www.hackerrank.com/challenges/capturing-non-capturing-groups/problem?isFullScreen=true\n",
        "\n",
        "\n",
        "(?:[o][k]){3,}\n",
        "\n",
        "###ML1_2: \n",
        "https://www.hackerrank.com/challenges/branch-reset-groups/problem?isFullScreen=true\n",
        "\n",
        "'/(?|(\\d\\d-\\d\\d-\\d\\d-\\d\\d$)|(^\\d\\d---\\d\\d---\\d\\d---\\d\\d)|(\\d\\d\\.\\d\\d\\.\\d\\d\\.\\d\\d)|(\\d\\d:\\d\\d:\\d\\d:\\d\\d))/'\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}